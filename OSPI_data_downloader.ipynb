{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowloading multiple S275 files from state web-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single file case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name ='C:/Users/jhernandez/Documents/S275/data_test.accdb'# change where this file gets dumped into \n",
    "url = 'http://www.k12.wa.us/safs/PUB/PER/1516/2015-2016_Final_S-275_Personnel_Database.accdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple file case using Beautiful Soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_url = \"http://www.k12.wa.us/safs/\"\n",
    "archive_url = \"http://www.k12.wa.us/safs/db.asp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(archive_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create bs object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content,'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all live links on web-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = soup.findAll('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only look at links that end with desired file type...in our case \".mdb\" and \".accdb\"...both access files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = [data_url + link['href'] for link in links if link['href'].endswith(('.mdb', '.accdb'))]\n",
    "print(data_links)\n",
    "type(data_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like there are duplicate links:\n",
    "### HTML table contains two link and its extracting all of them, the following is a quick fix.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=()\n",
    "i = set(data_links) \n",
    "for x in i:\n",
    "    link = str(x)       \n",
    "    l+=(x,)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together by creating a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "URL of the OSPI web-page which provides link to\n",
    "all the data files.\n",
    "In this example, we first crawl the webpage to extract\n",
    "all the links and then download the data.\n",
    "'''\n",
    " \n",
    "# specify the URL\n",
    "data_url = \"http://www.k12.wa.us/safs/\"\n",
    "archive_url = \"http://www.k12.wa.us/safs/db.asp\"\n",
    "\n",
    " \n",
    "def get_data_links():\n",
    "     \n",
    "    # create response object\n",
    "    r = requests.get(archive_url)\n",
    "     \n",
    "    # create beautiful-soup object\n",
    "    soup = BeautifulSoup(r.content,'html5lib')\n",
    "     \n",
    "    # find all links on web-page\n",
    "    links = soup.findAll('a')\n",
    " \n",
    "    # filter the link sending with .mp4\n",
    "    l_links = [data_url + link['href'] for link in links if link['href'].endswith(('.mdb', '.accdb'))]\n",
    "    \n",
    "    data_links=[]\n",
    "    #remove the duplication...there must be a better way!\n",
    "    i = set(l_links) \n",
    "    for x in i:\n",
    "        link = str(x)       \n",
    "        data_links+=(x,)\n",
    " \n",
    "    return data_links\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to create a loop to do the job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = get_data_links()   \n",
    "type(link)\n",
    "get_data_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = 'http://www.k12.wa.us/safs/PUB/PER/0405/2004-2005S275FinalForPublic.mdb'\n",
    "#test.split('/')[-1]   \n",
    "#'C:/Users/jhernandez/Documents/S275/'+test.split('/')[-1]\n",
    "data_links = get_data_links()\n",
    "type(data_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd = 'C:/Users/jhernandez/Documents/S275/'\n",
    "data_links = get_data_links()\n",
    "\n",
    "for link in data_links:\n",
    " \n",
    "        '''iterate through all links in data_links\n",
    "        and download them one by one'''\n",
    "         \n",
    "        # \n",
    "        file_name = hd+link.split('/')[-1]   \n",
    " \n",
    "        # \n",
    "        urllib.request.urlretrieve(link, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
